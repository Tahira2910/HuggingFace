{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wYKrj7pBA-TEx5TouEDjuDsg2I8_8zBM",
      "authorship_tag": "ABX9TyPESJnUTqn8dym3fYGG22nR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahira2910/NLP/blob/main/Text_Classification_Model_using_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-XU5-Gikc-N"
      },
      "outputs": [],
      "source": [
        "!pip install gensim -q\n",
        "!pip install matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string"
      ],
      "metadata": {
        "id": "2GGFw42qkraB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gensim.__version__"
      ],
      "metadata": {
        "id": "JrHl9BLElWne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "9xqcLWsMlWpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using gensim api, we can download pre trined wordvectors\n",
        "import gensim.downloader as api\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "id": "vvpN4UlNom9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading vector\n",
        "# wv = api.load('glove-twitter-50')"
      ],
      "metadata": {
        "id": "3S8PacoUom_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading vector\n",
        "# wv = api.load('word2vec-google-news-300')\n",
        "# wv.save('/content/drive/MyDrive/Vectors.kv')"
      ],
      "metadata": {
        "id": "ZmgdlWcHt1cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv['apple']"
      ],
      "metadata": {
        "id": "MdoCVqqQqb4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(wv['apple'])"
      ],
      "metadata": {
        "id": "rALhHYBOlWsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "Qum0jDVRrAzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = KeyedVectors.load('/content/drive/MyDrive/Vectors.kv')"
      ],
      "metadata": {
        "id": "Fx43CxT1tRZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity('apple', 'mango')"
      ],
      "metadata": {
        "id": "Uwm7Yat8qYK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv.similarity('apple', 'car')"
      ],
      "metadata": {
        "id": "ZjnwK1S-qYMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking similarity pairwise\n",
        "pairs = [\n",
        "    ('car', 'minivan'),\n",
        "    ('car', 'bicycle'),\n",
        "    ('car', 'airplane'),\n",
        "    ('car', 'cereal'),\n",
        "    ('car', 'communism')\n",
        "]\n",
        "for w1, w2 in pairs:\n",
        "  print('%r\\t%r\\t%.2f' % (w1,w2, wv.similarity(w1, w2)))"
      ],
      "metadata": {
        "id": "t8zHzQGXqYQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get  5 similar words to those specified in this list, with a score associated with it.\n",
        "print(wv.most_similar(positive = ['car', 'minivan'], topn = 5))"
      ],
      "metadata": {
        "id": "MSMsp9vwqYTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
      ],
      "metadata": {
        "id": "stK1bAdQOXX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Semantic regularities captured in word embeddings**"
      ],
      "metadata": {
        "id": "_mwrRNTSO4aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar(positive = ['woman', 'king'], negative = ['man'], topn = 3)"
      ],
      "metadata": {
        "id": "aQL2qETcOXbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['one', 'two', 'man', 'woman', 'table']"
      ],
      "metadata": {
        "id": "pKBZfP7ROXeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://deepai.org/dataset/text8"
      ],
      "metadata": {
        "id": "vHN_hiCUP3kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_vectors = np.array([wv[word] for word in words])\n",
        "# PCA: Principal Component Analysis-->reduces those 300 dimensions into 2 dim\n",
        "#  so that it is able to visualize\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(sample_vectors)\n",
        "result"
      ],
      "metadata": {
        "id": "Dx8uEiQZP3mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (12,12))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "  plt.annotate(word, xy = (result[i, 0], result[i,1]))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "bUHxytp0P3p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all the setences must have same length, for that we will take average..."
      ],
      "metadata": {
        "id": "RBSH7aNCOXgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NcjMJ0iKBXdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/train.csv\", error_bad_lines=False, engine=\"python\")"
      ],
      "metadata": {
        "id": "_-TiuuOXBJ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:500]\n",
        "data.toxic.value_counts()"
      ],
      "metadata": {
        "id": "qHaPBUyuHMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(500)"
      ],
      "metadata": {
        "id": "BaJEtfUWHMwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1IfqJBseH3e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import spacy"
      ],
      "metadata": {
        "id": "gBdJDqubHwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ],
      "metadata": {
        "id": "Cbeq0ykiHMzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # spacy small(sm) model\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "id": "qLfGsBlBHqvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_vec(sent):\n",
        "  vector_size = wv.vector_size   # value/dimension: 300\n",
        "  wv_res = np.zeros(vector_size)    # all vecotrs will be zeros\n",
        "  # print(wv_res)\n",
        "  ctr = 1\n",
        "  for w in sent:    # iterate through our sentence--> w\n",
        "    if w in wv:    # do we have that particular word present in our word2vec\n",
        "      ctr += 1    # we will count them. bcz for average we need to know how many in our sentenceS\n",
        "      wv_res += wv[w]    #  add vector representation of this particular word to zero vector\n",
        "  wv_res =  wv_res/ctr   # evetually added all the vectors associated with each word in the sentence(of the dataset) in\n",
        "  # this final vector--> wv_res....and then divide by the no of words-> ctr\n",
        "  return wv_res\n"
      ],
      "metadata": {
        "id": "R9stEPRoUgmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    #  Creating our token object, which is used to create documents with\n",
        "    # linguistic annotations.\n",
        "\n",
        "    doc = nlp(sentence)    # passing the text to nlp function(spacy model)\n",
        "  # print(doc), it will give tokens(word, symbol in eng language)\n",
        "  # doc--> object\n",
        "    # print(doc)\n",
        "    # print(type(doc))\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    # lemma--> root word for that particular english word, then make it lowercase and then strip left or right space if it has\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
        "\n",
        "    # print(mytokens)\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n"
      ],
      "metadata": {
        "id": "ORh3kht9Ugoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_vec('I am happy')"
      ],
      "metadata": {
        "id": "ObMHjO1mUgry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens'] = data['comment_text'].apply(spacy_tokenizer)"
      ],
      "metadata": {
        "id": "0S4iW_8BUh5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "OO7VHUNpUh8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get avg vectors\n",
        "data['vec'] = data['tokens'].apply(sent_vec)"
      ],
      "metadata": {
        "id": "Ed0fPMDqUh-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "-Wda_6IqOZyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['vec'].to_list()\n",
        "Y = data['toxic'].to_list()"
      ],
      "metadata": {
        "id": "IDmBneq3OZ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output: 300 dim vector\n",
        "X[0]"
      ],
      "metadata": {
        "id": "x8vERk1sOZ3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# stratify=ylabels--> to maintain the balance of toxic and non toxic data of y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)\n"
      ],
      "metadata": {
        "id": "AFBsnRY_UiCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()"
      ],
      "metadata": {
        "id": "3Tyo9v8mPdBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "r2U7GghsPdET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "ST2JjO6MPqg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = classifier.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
        "print(\"Logistic Regression Precision:\", metrics.precision_score(y_test, predicted))\n",
        "print(\"Logistic Regression Recall:\", metrics.recall_score(y_test, predicted))\n"
      ],
      "metadata": {
        "id": "hqlbzfzaPqkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "noM78ACSPqnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}