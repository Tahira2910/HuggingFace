{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTIa2De+uiqoCxQamiIO6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahira2910/NLP/blob/main/Feature_Engineering_Technique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text to Numerical Representation**"
      ],
      "metadata": {
        "id": "NeL7vfgF7zSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe6GqzAU7FQI"
      },
      "outputs": [],
      "source": [
        "!pip install spacy -q\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy -q\n",
        "!pip install pandas -q"
      ],
      "metadata": {
        "id": "Xs2my51G8FxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "\n",
        "import string\n",
        "import spacy  # for tokenization purpose\n",
        "np.random.seed(42)\n"
      ],
      "metadata": {
        "id": "RfmwsjbS7Ub6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/train.csv\", error_bad_lines=False, engine=\"python\")"
      ],
      "metadata": {
        "id": "vAxFZAds8nfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[:500]\n",
        "data.toxic.value_counts()"
      ],
      "metadata": {
        "id": "B-oGwhmj4LFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(500)"
      ],
      "metadata": {
        "id": "RcokGBfw8E7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # spacy small(sm) model\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "id": "433Le980Vay3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ],
      "metadata": {
        "id": "UtJMpIy2V9eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    #  Creating our token object, which is used to create documents with\n",
        "    # linguistic annotations.\n",
        "\n",
        "    doc = nlp(sentence)    # passing the text to nlp function(spacy model)\n",
        "  # print(doc), it will give tokens(word, symbol in eng language)\n",
        "  # doc--> object\n",
        "    # print(doc)\n",
        "    # print(type(doc))\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    # lemma--> root word for that particular english word, then make it lowercase and then strip left or right space if it has\n",
        "    mytokens = [ word.lemma_.lower() for word in doc ]\n",
        "\n",
        "    # print(mytokens)\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n"
      ],
      "metadata": {
        "id": "lLV_WFyFV9hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I am eating apple?\"\n",
        "spacy_tokenizer(sentence)\n",
        "# eating(lemma or root form)--> eat"
      ],
      "metadata": {
        "id": "JNKtQ4YQek5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CountVectorizer**"
      ],
      "metadata": {
        "id": "riABx68PhN95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# giving tokenizer obj to CountVectorizer\n",
        "# declaring tokenizer function(spacy_tokenizer)\n",
        "count_vector = CountVectorizer(tokenizer = spacy_tokenizer) #passing spacy_tokenizer function definer earlier\n",
        "# tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n"
      ],
      "metadata": {
        "id": "RvxjwL-tek8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count_vector.fit_transform([\"I am eating apple\",\"I am playing cricket\"]).toarray()\n"
      ],
      "metadata": {
        "id": "wMg2IIkXgcUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "count_vector.get_feature_names_out()"
      ],
      "metadata": {
        "id": "DC_lSSjEgcXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector.vocabulary_"
      ],
      "metadata": {
        "id": "_1Vi0MuQgcgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = data['comment_text'] # features we want to analyze\n",
        "ylabels = data['toxic'] # the labels, or answers, we want to test against\n",
        "# stratify=ylabels--> to maintain the balance of toxic and non toxic data of y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, stratify=ylabels)\n"
      ],
      "metadata": {
        "id": "cHMMGq0Rityb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()"
      ],
      "metadata": {
        "id": "BBB0DJs2it06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if a transformation is working on training data\n",
        "# it also neends to work on testing data"
      ],
      "metadata": {
        "id": "AhNP5jjCm39S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting dataset to count vectorizer just as we fit 2 sentences to count vectorizer\n",
        "X_train_vectors = count_vector.fit_transform(X_train)\n",
        "X_test_vectors = count_vector.transform(X_test)"
      ],
      "metadata": {
        "id": "O2vYm2Oxit4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st value--> total examples, 2nd value--> total unique words(length of each example)\n",
        "X_train_vectors.shape"
      ],
      "metadata": {
        "id": "dYa8tJsHnCVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_vectors.shape"
      ],
      "metadata": {
        "id": "nTKPxLIsmlvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectors.toarray()"
      ],
      "metadata": {
        "id": "dOv1p1WYmE1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train_vectors, y_train)"
      ],
      "metadata": {
        "id": "7_7THpTVmE3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = classifier.predict(X_test_vectors)\n",
        "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
        "print(\"Logistic Regression Precision:\", metrics.precision_score(y_test, predicted))\n",
        "print(\"Logistic Regression Recall:\", metrics.recall_score(y_test, predicted))\n"
      ],
      "metadata": {
        "id": "pVD_K6hpmE7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorizer**"
      ],
      "metadata": {
        "id": "M6Uhfkq01tqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
        "X_train_vectors = tfidf_vector.fit_transform(X_train)\n",
        "X_test_vectors = tfidf_vector.transform(X_test)"
      ],
      "metadata": {
        "id": "PVE8LKrx1pmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()"
      ],
      "metadata": {
        "id": "4ccwp1VV1ppV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train_vectors, y_train)"
      ],
      "metadata": {
        "id": "hH4GKkvu1psa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = classifier.predict(X_test_vectors)\n",
        "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
        "print(\"Logistic Regression Precision:\", metrics.precision_score(y_test, predicted))\n",
        "print(\"Logistic Regression Recall:\", metrics.recall_score(y_test, predicted))\n"
      ],
      "metadata": {
        "id": "o3TUImpb7LpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}