{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHoudUeUIkYwL/tVQ3IhsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahira2910/NLP/blob/main/Text_Classification_Model_Using_Sentence_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QfeLEH_R6YQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(2022)"
      ],
      "metadata": {
        "id": "FXwjpLNeTWSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "fRqJWjjkTWTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/train.csv\")"
      ],
      "metadata": {
        "id": "KdkLG7kWTWXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1).reset_index()\n",
        "data = data[:500]\n",
        "data.toxic.value_counts()"
      ],
      "metadata": {
        "id": "_m-RL_2Bf06s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(300)"
      ],
      "metadata": {
        "id": "iRpWZauVVAx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import spacy\n",
        " import string\n",
        " nlp = spacy.load(\"en_core_web_sm\")\n",
        " stop_words = nlp.Defaults.stop_words\n",
        " print(stop_words)"
      ],
      "metadata": {
        "id": "YOQPEIZqVA1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ],
      "metadata": {
        "id": "cF0OKQcZdkG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    #  Creating our token object, which is used to create documents with\n",
        "    # linguistic annotations.\n",
        "\n",
        "    doc = nlp(sentence)    # passing the text to nlp function(spacy model)\n",
        "  # print(doc), it will give tokens(word, symbol in eng language)\n",
        "  # doc--> object\n",
        "    # print(doc)\n",
        "    # print(type(doc))\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    # lemma--> root word for that particular english word, then make it lowercase and then strip left or right space if it has\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
        "\n",
        "    # print(mytokens)\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n",
        "\n",
        "    sentence = \" \".join(mytokens)\n",
        "    # return preprocessed list of tokens\n",
        "    return sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "_3vJmAC1dkJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_tokenizer(\"I am Happy\")"
      ],
      "metadata": {
        "id": "OeWhZAwCe6eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokenize'] = data['comment_text'].apply(spacy_tokenizer)"
      ],
      "metadata": {
        "id": "MTPy0DRSfBmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after removing stop words and punctuation marks\n",
        "data.head()"
      ],
      "metadata": {
        "id": "iodCJQtwfBpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a new col-->embeddings and store encoded tokenize col\n",
        "data['embeddings'] = data['tokenize'].apply(model.encode)\n",
        "# last col can be called as feature vector"
      ],
      "metadata": {
        "id": "wKbMUbNEe6hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "smVoKrr_kFL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['embeddings'].to_list()\n",
        "y = data['toxic'].to_list()"
      ],
      "metadata": {
        "id": "IBaxY1Yyj6Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "id": "vM-GC-PLlITo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, stratify=y)\n"
      ],
      "metadata": {
        "id": "n6uBmwBVlIWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "LR = LogisticRegression()\n",
        "LR.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Knvp_MPumnJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = LR.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
        "print(\"Logistic Regression Precision:\", metrics.precision_score(y_test, predicted))\n",
        "print(\"Logistic Regression Recall:\", metrics.recall_score(y_test, predicted))\n"
      ],
      "metadata": {
        "id": "cETuna6ImnLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDHXd8rnmnOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}